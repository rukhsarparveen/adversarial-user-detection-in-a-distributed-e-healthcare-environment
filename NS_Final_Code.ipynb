{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93591477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e17ed50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('diab_clean_version.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6658e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff5f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 0'], inplace = True , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3da1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning 0 if less than 30 else 1\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 0 if x=='<30' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f28963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving readmitted 0 and value in file\n",
    "data.to_csv('diab_clean_version.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4dd7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = data.drop('readmitted', axis=1) # Feature matrix\n",
    "y = data['readmitted'] # Target variable\n",
    "X = pd.get_dummies(X) # One-hot encode categorical features\n",
    "X = X.fillna(0) # Replace missing values with 0\n",
    "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1181d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the classes using RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eee07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a05111e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the hyperparameters using GridSearchCV\n",
    "param_grid = {'n_estimators': [100, 200, 300],\n",
    "              'max_depth': [10, 20, 30],\n",
    "              'min_samples_split': [2, 5, 10]}\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b04171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=30, n_estimators=300, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=30, n_estimators=300, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=30, n_estimators=300, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and train the random forest classifier with the best hyperparameters\n",
    "model = RandomForestClassifier(random_state=42, **best_params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72d11903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac988309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9527518321597931\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     17435\n",
      "           1       0.99      0.91      0.95     17360\n",
      "\n",
      "    accuracy                           0.95     34795\n",
      "   macro avg       0.96      0.95      0.95     34795\n",
      "weighted avg       0.96      0.95      0.95     34795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy, precision, recall, and f1-score\n",
    "baseline_model_accuracy = accuracy_score(y_test, y_pred)\n",
    "baseline_model_report = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", baseline_model_accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(baseline_model_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28de49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users among which we will split the data\n",
    "n_users = 5\n",
    "\n",
    "# Shuffle the data randomly\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# Split the data into n_users subsets each of same size\n",
    "users_data = np.array_split(data, n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79c4eaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19611, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8fe58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize and save the model\n",
    "with open('initial_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "# Share the initial model with all users\n",
    "for i in range(n_users):\n",
    "    with open('initial_model.pkl', 'rb') as f:\n",
    "        model_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c047ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is training each user locally on their subset and returning accuracy and Classification-report\n",
    "def train_model(model, local_data):\n",
    "    # Load the initial ML model from a file\n",
    "    with open('initial_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    X_train_i = local_data.drop('readmitted', axis=1) # Feature matrix\n",
    "    y_train_i = local_data['readmitted'] # Target variable\n",
    "    X_train_i = pd.get_dummies(X_train_i) # One-hot encode categorical features\n",
    "    X_train_i = X_train_i.fillna(0) # Replace missing values with 0\n",
    "    X_new = SelectKBest(chi2, k=10).fit_transform(X_train_i, y_train_i)\n",
    "    \n",
    "    \n",
    "    # balance the classes using RandomOverSampler\n",
    "    oversampler = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X_new, y_train_i)\n",
    "\n",
    "        \n",
    "   \n",
    "    # split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy_i = accuracy_score(y_test, y_pred)\n",
    "    report_i =  classification_report(y_test, y_pred)\n",
    "    return accuracy_i, report_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a910ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store locally trained model accuracy\n",
    "trained_models = []\n",
    "# List to store locally trained model classification-report\n",
    "report_data =[]\n",
    "\n",
    "# Loop over each user\n",
    "for i in range(n_users):\n",
    "    # Train the received model on local data\n",
    "    local_model, report = train_model(model, users_data[i])\n",
    "    \n",
    "    # Add accuracy and classification-report to the list\n",
    "    trained_models.append(local_model)\n",
    "    report_data.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d9d7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of user0 is 0.961688031110471\n",
      "accuracy of user1 is 0.9640856198822009\n",
      "accuracy of user2 is 0.9671118770644836\n",
      "accuracy of user3 is 0.9685976484083739\n",
      "accuracy of user4 is 0.9623292595255212\n"
     ]
    }
   ],
   "source": [
    "#printing accuracy of all the users\n",
    "for i in range(n_users):\n",
    "    print(f\"accuracy of user{i} is {trained_models[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7cd050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      3435\n",
      "           1       1.00      0.93      0.96      3508\n",
      "\n",
      "    accuracy                           0.96      6943\n",
      "   macro avg       0.96      0.96      0.96      6943\n",
      "weighted avg       0.96      0.96      0.96      6943\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      3429\n",
      "           1       1.00      0.93      0.96      3532\n",
      "\n",
      "    accuracy                           0.96      6961\n",
      "   macro avg       0.97      0.96      0.96      6961\n",
      "weighted avg       0.97      0.96      0.96      6961\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      3469\n",
      "           1       1.00      0.94      0.97      3494\n",
      "\n",
      "    accuracy                           0.97      6963\n",
      "   macro avg       0.97      0.97      0.97      6963\n",
      "weighted avg       0.97      0.97      0.97      6963\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      3478\n",
      "           1       1.00      0.94      0.97      3496\n",
      "\n",
      "    accuracy                           0.97      6974\n",
      "   macro avg       0.97      0.97      0.97      6974\n",
      "weighted avg       0.97      0.97      0.97      6974\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      3522\n",
      "           1       1.00      0.93      0.96      3433\n",
      "\n",
      "    accuracy                           0.96      6955\n",
      "   macro avg       0.97      0.96      0.96      6955\n",
      "weighted avg       0.96      0.96      0.96      6955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing classification-report of all the users\n",
    "for i in range(n_users):\n",
    "    print(report_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54d57f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is calculating avg of all the locally trained models and return as global model\n",
    "def average_models(models):\n",
    "    global_model = None\n",
    "    num_models = len(models)\n",
    "    \n",
    "    # Compute the sum of all models\n",
    "    for model in models:\n",
    "        if global_model is None:\n",
    "            global_model = model\n",
    "        else:\n",
    "            global_model += model\n",
    "    \n",
    "    # Divide the sum by the number of models to get the average\n",
    "    global_model /= num_models\n",
    "    \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7199f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = average_models(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "535b4593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9647624871982101"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy of global model\n",
    "global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12a18616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012010655038417029"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Difference between baseline global model and Merged global model\n",
    "diff_base_merge = baseline_model_accuracy - global_model\n",
    "abs(diff_base_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff421c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of user0 : 0.008936198950677876\n",
      "accuracy of user1 : 0.011333787722407784\n",
      "accuracy of user2 : 0.014360044904690561\n",
      "accuracy of user3 : 0.015845816248580835\n",
      "accuracy of user4 : 0.009577427365728086\n"
     ]
    }
   ],
   "source": [
    "#Difference between each baseline model with each locally trained model\n",
    "\n",
    "for i in range(n_users):\n",
    "    diff_base_local = abs(baseline_model_accuracy - trained_models[i])\n",
    "    print(f\"accuracy of user{i} : {diff_base_local}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a304a8",
   "metadata": {},
   "source": [
    "## Make user[1] adversary by poisoning its dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf316339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to poison the dataset:\n",
    "def poison(data, fraction):\n",
    "    #total_data = data.shape[0]\n",
    "    user2_indices = range(0, 19610)\n",
    "    modified_data = int(len(user2_indices) * fraction)\n",
    "    indices = np.random.choice(user2_indices, modified_data, replace= False)\n",
    "    data.loc[indices, 'readmitted'] = data.loc[indices, 'readmitted'].apply(lambda x: 0 if x== 1 else 1)\n",
    "    #data.loc[indices, 'readmitted'] = data.loc[indices, 'readmitted'].apply(lambda x: '>30' if x== '<30' else '<30') \n",
    "    #data.to_csv(\"check.csv\",index = False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "183c9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetting the index of the user, whose data we are poisoning\n",
    "users_data[1] = users_data[1].reset_index(drop=True)\n",
    "adversarial_data = poison(users_data[1] , 0.4) #40% data poison\n",
    "#adversarial_data = poison(users_data[1] , 0.1) #10% data poison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1c33b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store locally trained models accuracy\n",
    "poisoned_trained_models = []\n",
    "# List to store locally trained models classification-report\n",
    "poisoned_report_data = []\n",
    "\n",
    "# Loop over each participating user\n",
    "for i in range(n_users):\n",
    "    if i != 1:\n",
    "        # Train the received model on local data\n",
    "        local_model,poisoned_report = train_model(model, users_data[i])\n",
    "    else:\n",
    "        local_model, poisoned_report = train_model(model, adversarial_data)\n",
    "        \n",
    "    \n",
    "    \n",
    "   # Add accuracy and classification-report to the list\n",
    "    poisoned_trained_models.append(local_model)\n",
    "    poisoned_report_data.append(poisoned_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaaea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of user0 is 0.9576551922799943\n",
      "accuracy of user1 is 0.5526548672566372\n",
      "accuracy of user2 is 0.966393795777682\n",
      "accuracy of user3 is 0.968741038141669\n",
      "accuracy of user4 is 0.9629043853342919\n"
     ]
    }
   ],
   "source": [
    "#printing accuracy of all the user, here user[1] accuracy drop, so it is possible that it is adversarial user:\n",
    "for i in range(n_users):\n",
    "    print(f\"accuracy of user{i} is {poisoned_trained_models[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "313176be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user0 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      3435\n",
      "           1       0.99      0.92      0.96      3508\n",
      "\n",
      "    accuracy                           0.96      6943\n",
      "   macro avg       0.96      0.96      0.96      6943\n",
      "weighted avg       0.96      0.96      0.96      6943\n",
      "\n",
      "user1 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.56      0.55      2225\n",
      "           1       0.56      0.54      0.55      2295\n",
      "\n",
      "    accuracy                           0.55      4520\n",
      "   macro avg       0.55      0.55      0.55      4520\n",
      "weighted avg       0.55      0.55      0.55      4520\n",
      "\n",
      "user2 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      3469\n",
      "           1       1.00      0.94      0.97      3494\n",
      "\n",
      "    accuracy                           0.97      6963\n",
      "   macro avg       0.97      0.97      0.97      6963\n",
      "weighted avg       0.97      0.97      0.97      6963\n",
      "\n",
      "user3 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      3478\n",
      "           1       1.00      0.94      0.97      3496\n",
      "\n",
      "    accuracy                           0.97      6974\n",
      "   macro avg       0.97      0.97      0.97      6974\n",
      "weighted avg       0.97      0.97      0.97      6974\n",
      "\n",
      "user4 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      3522\n",
      "           1       1.00      0.93      0.96      3433\n",
      "\n",
      "    accuracy                           0.96      6955\n",
      "   macro avg       0.97      0.96      0.96      6955\n",
      "weighted avg       0.97      0.96      0.96      6955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing classification-report of all the user, here user[1] clssification is very poor:\n",
    "for i in range(n_users):\n",
    "    print(f\"user{i} classification report:\")\n",
    "    print(poisoned_report_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b8f2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_global_model = average_models(poisoned_trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf37d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global accuracy after data poisioning :0.8816698557580548\n"
     ]
    }
   ],
   "source": [
    "#average accuracy of global model:\n",
    "#Note: here accuracy decreased and one of the reason coud be adversarial user\n",
    "\n",
    "print(f\"global accuracy after data poisioning :{poisoned_global_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab29a82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0710819764017383"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Difference between baseline global model and Poisoned global model\n",
    "diff_base_poisoned_merge = baseline_model_accuracy - poisoned_global_model\n",
    "abs(diff_base_poisoned_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "628a78e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy difference of user0 with baseline model : 0.004903360120201206\n",
      "accuracy difference of user1 with baseline model : 0.40009696490315594\n",
      "accuracy difference of user2 with baseline model : 0.013641963617888941\n",
      "accuracy difference of user3 with baseline model : 0.01598920598187592\n",
      "accuracy difference of user4 with baseline model : 0.010152553174498813\n"
     ]
    }
   ],
   "source": [
    "#Difference between each baseline model with each locally trained model after poisoning\n",
    "\n",
    "for i in range(n_users):\n",
    "    diff_base_poison_local = abs(baseline_model_accuracy - poisoned_trained_models[i])\n",
    "    print(f\"accuracy difference of user{i} with baseline model : {diff_base_poison_local}\")\n",
    "#Here you can notice that user[1] performing different from everyone. So we will remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5625c",
   "metadata": {},
   "source": [
    "##   Remove Adversarial user and check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a03c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the accuracy of locally trained models\n",
    "without_adversarial_trained_models = []\n",
    "# List to store the classification-report of locally trained models\n",
    "without_adversarial_report = []\n",
    "\n",
    "# Loop over each participating user: we have included all the user except user[1]\n",
    "for i in range(n_users):\n",
    "    if i != 1:\n",
    "        # Train the received model on local data\n",
    "        local_model, adverse_report = train_model(model, users_data[i])\n",
    "        #local_model, adverse_report = train_model(model, adversarial_data)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Add the trained model to the list of locally trained models\n",
    "    without_adversarial_trained_models.append(local_model)\n",
    "    without_adversarial_report.append(adverse_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6f7f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of user0 : 0.9619760910269336\n",
      "accuracy of user2 : 0.9666810282924027\n",
      "accuracy of user3 : 0.9680240894751936\n",
      "accuracy of user4 : 0.9617541337167506\n"
     ]
    }
   ],
   "source": [
    "#printing the accuracy of all the user except user[1]\n",
    "for i in range(n_users):\n",
    "    if i != 1:\n",
    "        print(f\"accuracy of user{i} : {without_adversarial_trained_models[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9938ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the global average without the user which we suspect of being adversarial: in our case that user is user[1]\n",
    "without_adversarial_global_model = average_models(without_adversarial_trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d30cad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of global model after removing adversary user: 0.9640822867076428\n"
     ]
    }
   ],
   "source": [
    "#notice that after removing user[1] our accuracy has improved as it was before poisoning \n",
    "\n",
    "print(f\"accuracy of global model after removing adversary user: {without_adversarial_global_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f3608c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011330454547849755"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Difference between baseline global model and adverse user removed global model\n",
    "diff_base_without_adversarialUser_merge = baseline_model_accuracy - without_adversarial_global_model\n",
    "abs(diff_base_without_adversarialUser_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac9aee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of user0 : 0.004903360120201206\n",
      "accuracy of user2 : 0.013641963617888941\n",
      "accuracy of user3 : 0.01598920598187592\n",
      "accuracy of user4 : 0.010152553174498813\n"
     ]
    }
   ],
   "source": [
    "#Difference between each baseline model with each locally trained model except adversarial user\n",
    "for i in range(n_users):\n",
    "    if i != 1: \n",
    "        diff_base_poison_local = abs(baseline_model_accuracy - poisoned_trained_models[i])\n",
    "        print(f\"accuracy of user{i} : {diff_base_poison_local}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a1282f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.to_csv('diab_clean_version.csv',index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c4ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad35c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
